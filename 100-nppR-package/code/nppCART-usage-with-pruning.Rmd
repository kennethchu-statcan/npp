---
title: "nppCART -- usage (with pruning)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{nppCART-usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
    )
```

This vignette demonstrates how to use the pruning functionality of __nppCART__,
which can be invoked if the probability sample is equipped
with bootstrap weights.

Outline of this vignette:

*  Section 1: create data for a synthetic population,

*  Section 2: generate a pair of non-probability sample and probability sampe
   from the synthetic population,

*  Section 3: estimate the self-selection propensities
   -- according to the unpruned and pruned trees, respectively --
   for the units in the non-probability sample using __nppCART__.

### Section 1. Generate the synthetic population data frame.

First, we load the required R packages.
```{r}
library(nppR);
```

The following code segment uses the function
__nppR::get.synthetic.population( )__
in order to generate data for a synthetic population
and store the data in the data frame __DF.population__.
This synthetic population is designed so that
the unpruned tree will over-fit, which can then serve
to showcases the utility of pruning in such scenarios.

For reproducibility, we supply a randomization seed.

```{r}
DF.population <- nppR::get.synthetic.population(
    population.size = 10000,
    seed            = 7654321
    );
```

Here are the first few rows of **DF.population**:

```{r}
knitr::kable(head(DF.population), align = "c", row.names = FALSE);
```

We remark that _y_ is intended to be the target variable,
while _x~1~_, _x~2~_, and _x~3~_ are the predictor variables.
The **true.propensity** is, of course, unknown in practice.
Here are the structure and summary statistics of __DF.population__:

```{r}
utils::str(DF.population);
base::summary(DF.population);
```

### Section 2. Generate data frames for the non-probability and probability samples.

We now generate a pair of non-probability and probablity samples
using the function __nppR::get.npp.samples()__.

The non-probability sample is a Poisson sample from the synthetic population
each of whose units is selected -- independently from all other units --
according to its own unit-specific (true) self-selection propensity.

The probablity sample is a simple random sample (SRS)
from the synthetic population.

The function __nppR::get.npp.samples()__ allows the specification of the
selection probablity of the SRS through the input parameter __prob.selection__,
and the number of sets of bootstrap weights through __n.replicates__.

We store data for the non-probability sample
in the data frame __DF.non.probability__,
and that of the probability sample in __DF.probability__.

```{r}
n.replicates   <- 200;
prob.selection <- 0.2;

list.samples  <- nppR::get.npp.samples(
    DF.population  = DF.population,
    prob.selection = prob.selection,
    n.replicates   = n.replicates,
    seed           = 7654321
    );
DF.non.probability <- list.samples[['DF.non.probability']];
DF.probability     <- list.samples[['DF.probability'    ]];
```

Here are the first few rows of __DF.non.probability__:

```{r}
knitr::kable(head(DF.non.probability), align = "c", row.names = FALSE);
```

And, the first few rows of __DF.probability__:

```{r}
knitr::kable(head(DF.probability), align = "c", row.names = FALSE);
```

### Section 3. Compute estimated propensities via __nppCART__, with an without pruning.

Now that we have the two input data sets ready
(i.e., the data frames __DF.non.probability__ and __DF.probability__),
we are ready to use __nppCART__ to estimate the self-selection propensities
for the units in the non-probability sample, using the auxiliary information
fournished by the probability sample.

We start by instantiating an __nppCART__ object,
with the two input data sets as follows:

```{r}
my.nppCART <- nppR::nppCART(
    np.data                   = DF.non.probability,
    p.data                    = DF.probability,
    predictors                = c("x1","x2","x3"),
    sampling.weight           = "design.weight",
    bootstrap.weights         = paste0("repweight",seq(1,n.replicates)),
    impurity                  = 'entropy',
    min.cell.size.np          = 1,
    min.cell.size.p           = 1,
    min.impurity              = 1e-10,
    n.levels.approx.threshold = 4
    );
```

Pruning is automatically triggered by a non-NULL value of __bootstrap.weights__.
On the other hand, if __bootstrap.weights__ is NULL or omitted
in the call to __nppCART__, then no pruning is done.

Next, we call the __nppCART$grow( )__ method to grow the classification tree
according to the tree-growing algorithm of __nppCART__.

```{r}
my.nppCART$grow();
```

Once the tree growing is complete, we can examine the fully grown (unpruned)
tree with the __nppCART$print( )__ method:

```{r}
my.nppCART$print( FUN.format = function(x) {return(round(x,digits=3))} );
```

We next extract the estimated propensities, as a data frame,
using the __nppCART$get_npdata_with_propensity( )__ method:

```{r}
DF.npdata.estimated.propensity <- my.nppCART$get_npdata_with_propensity();
```

Here are the first few rows of the data frame returned by
 __nppCART$get_npdata_with_propensity( )__:

```{r}
knitr::kable(head(DF.npdata.estimated.propensity), align = "c", row.names = FALSE);
```

The first five columns (unit.ID, y, x1, x2, x3) is just
the input data frame __DF.non.probability__.
The rest of the columns are results of __nppCART__

The columns __nodeID__, __propensity__, __np.count__, __p.weight__, and	__impurity__
refer to the unpruned tree.

*  __nodeID__ indicates the ID of the terminal node (leaf) of the unpruned tree
   containing the non-probability sample unit of each given row,
*  __propensity__ is the self-selection propensity estimate of the terminal node,
*  __np.count__ is the number of non-probability sample units belonging
   to the terminal node,
*  __p.weight__ is the sum of the design sampling weights
   of the probablity sample units belonging to the terminal node, and
*  __impurity__ is the impurity of the termimal node.

The columns
__nodeID.pruned__, __propensity.pruned__, __np.count.pruned__,
__p.weight.pruned__, and	__impurity.pruned__
contain the counterpart nppCART results corresponding
to the optimally pruned tree.

We next extract the pruning sub-tree hierarchy in tabular format,
using the __nppCART$get_impurities_alphas_AICs( )__ method:

```{r}
DF.nppCART.impurity.alpha.AIC <- my.nppCART$get_impurities_alphas_AICs();
```

Here are the first few rows of the data frame returned by
 __nppCART$get_impurities_alphas_AICs( )__:

```{r}
knitr::kable(head(DF.nppCART.impurity.alpha.AIC), align = "c", row.names = FALSE);
```

```{r, echo = FALSE}
plot.impurity.alpha.AIC <- function(
    DF.input = NULL
    ) {

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    my.x.breaks   <- seq(0.032,0.034,0.001);
    my.y.breaks   <- seq(0,200,4);
    my.size.line  <- 0.5;
    my.size.point <- 1.3;
    textsize.axis <- 13;

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    index.min.subtree <- DF.input[which( DF.input[,'AIC'] == min(DF.input[,'AIC']) ),'index.subtree'];

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    my.ggplot.impurity <- ggplot(data = NULL) + theme_bw();
    my.ggplot.impurity <- my.ggplot.impurity + theme(
        axis.title.x     = element_text(size = textsize.axis,  face = "bold"),
        axis.title.y     = element_text(size = textsize.axis,  face = "bold"),
        axis.text.x      = element_text(size = textsize.axis,  face = "bold"),
        axis.text.y      = element_text(size = textsize.axis,  face = "bold"),
        panel.grid.major = element_line(colour="gray", linetype=2, size=0.25),
        panel.grid.minor = element_line(colour="gray", linetype=2, size=0.25)
        );

    my.ggplot.impurity <- my.ggplot.impurity + labs(
        title    = NULL,
        subtitle = NULL
        );

    my.ggplot.impurity <- my.ggplot.impurity + xlab(
        label = "tree impurity"
        );

    my.ggplot.impurity <- my.ggplot.impurity + ylab(
        label = "sub-tree index"
        );

    my.ggplot.impurity <- my.ggplot.impurity + geom_point(
        data    = DF.input,
        mapping = aes(x = tree.impurity, y = index.subtree),
        alpha   = 0.9,
        size    = my.size.point,
        colour  = "black"
        );

    my.ggplot.impurity <- my.ggplot.impurity + geom_line(
        data        = DF.input,
        mapping     = aes(x = tree.impurity, y = index.subtree),
        orientation = "y",
        alpha       = 0.5,
        size        = my.size.line,
        colour      = "black"
        );

    my.ggplot.impurity <- my.ggplot.impurity + geom_hline(
        yintercept = index.min.subtree,
        colour     = "red"
        );

    my.ggplot.impurity <- my.ggplot.impurity + scale_x_continuous(
        limits = NULL,
        breaks = my.x.breaks
        );

    my.ggplot.impurity <- my.ggplot.impurity + scale_y_continuous(
        limits = NULL,
        breaks = my.y.breaks
        );

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    my.ggplot.alpha <- ggplot(data = NULL) + theme_bw();
    my.ggplot.alpha <- my.ggplot.alpha + theme(
        axis.title.x     = element_text(size = textsize.axis,  face = "bold"),
        axis.text.x      = element_text(size = textsize.axis,  face = "bold"),
        axis.title.y     = element_blank(),
        axis.text.y      = element_blank(),
        panel.grid.major = element_line(colour="gray", linetype=2, size=0.25),
        panel.grid.minor = element_line(colour="gray", linetype=2, size=0.25)
        );

    my.ggplot.alpha <- my.ggplot.alpha + labs(
        title    = NULL,
        subtitle = NULL
        );

    my.ggplot.alpha <- my.ggplot.alpha + xlab(
        label = "alpha (complex penalty weight)"
        );

    my.ggplot.alpha <- my.ggplot.alpha + geom_step(
        data    = DF.input,
        mapping = aes(x = alpha, y = index.subtree),
        alpha   = 0.9,
        size    = my.size.point,
        colour  = "black"
        );

    my.ggplot.alpha <- my.ggplot.alpha + geom_hline(
        yintercept = index.min.subtree,
        colour     = "red"
        );

    my.ggplot.alpha <- my.ggplot.alpha + scale_y_continuous(
        limits = NULL,
        breaks = my.y.breaks
        );

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    my.ggplot.AIC <- ggplot(data = NULL) + theme_bw();
    my.ggplot.AIC <- my.ggplot.AIC + theme(
        axis.title.x     = element_text(size = textsize.axis,  face = "bold"),
        axis.text.x      = element_text(size = textsize.axis,  face = "bold"),
        axis.title.y     = element_blank(),
        axis.text.y      = element_blank(),
        panel.grid.major = element_line(colour="gray", linetype=2, size=0.25),
        panel.grid.minor = element_line(colour="gray", linetype=2, size=0.25)
        );

    my.ggplot.AIC <- my.ggplot.AIC + labs(
        title    = NULL,
        subtitle = NULL
        );

    my.ggplot.AIC <- my.ggplot.AIC + geom_point(
        data    = DF.input,
        mapping = aes(x = AIC, y = index.subtree),
        alpha   = 0.9,
        size    = my.size.point,
        colour  = "black"
        );

    my.ggplot.AIC <- my.ggplot.AIC + geom_line(
        data        = DF.input,
        mapping     = aes(x = AIC, y = index.subtree),
        orientation = "y",
        alpha       = 0.5,
        size        = my.size.line,
        colour      = "black"
        );

    my.ggplot.AIC <- my.ggplot.AIC + geom_hline(
        yintercept = index.min.subtree,
        colour     = "red"
        );

    my.ggplot.AIC <- my.ggplot.AIC + scale_y_continuous(
        limits = NULL,
        breaks = my.y.breaks
        );

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    my.cowplot <- cowplot::plot_grid(
        my.ggplot.impurity,
        my.ggplot.alpha,
        my.ggplot.AIC,
        nrow       = 1,
        align      = "h",
        rel_widths = c(2,3,2)
        );

    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ###
    return( my.cowplot );

    }
```

As a diagnostics of the pruning, we plot
the tree impurity, alpha (complex penality weight) and AIC
against the sub-tree index for the sub-trees
in the pruning sub-tree hierarchy.

```{r, echo = FALSE, fig.height = 3, fig.width = 12}
my.cowplot <- plot.impurity.alpha.AIC(
    DF.input = DF.nppCART.impurity.alpha.AIC
    );
my.cowplot;
```

Note the expected monotonicity of tree impurity and alpha as functions
of the sub-tree index.
Note also that, as expected, as the sub-tree index increases,
the AIC first descreases and then increases.
The horizontal red line indicates the sub-tree index at which the AIC
is minimized; the corresponding sub-tree is the optimally pruned tree.
The estimates of the optimally pruned tree are reported in the output
data frame of __nppR::get_npdata_with_propensity( )__
under the columns whose names have the suffix ".pruned".
