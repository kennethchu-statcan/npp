---
title: "nppCART -- usage"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{nppCART-usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
    )
```
This vignette demonstrates how to use the R package **stcCropYield**
to do the following:

*  train the early-season parcel-level crop yield prediction model:

   *  based on historical yield, NDVI and weather measurement data
   *  uses the function **stcCropYield::rollingWindowForwardValidation(.)**
   *  normally takes place during the early months of the reference year,
      when the preceding year's insurance (crop yield and harvested area)
      data have become avavilable
   *  saves the trained model to file.

*  generate parcel-level crop yield predictions using a trained model
   previously saved to file.

   *  based on NDVI and weather data -- up to end of July --
      of the current growing season
   *  uses the function **stcCropYield::crop.yield.predict(.)**

In what follows, we pretend that we are in the production cycle 2021.

### Step 0. Generate synthetic data sets.

Obviously, in practice, there won't be a **Step 0** of synthetic data set
generation.
In this vignette, we generate in this step the two (synthetic) input data frames
needed for Step 1 and Step 2 below:

*  **DF.training**

*  **DF.production**

```{r}
base::set.seed(7654321);
```

```{r}
library(ggplot2);
library(nppR);
library(knitr);
```
The following code segment generates the two aforementioned data frames:
```{r}
population.size <- 10000;

temp.centres <- c(0.5,1.5,2.5);

c1 <- sample(x = temp.centres, size = population.size, replace = TRUE);
c2 <- sample(x = temp.centres, size = population.size, replace = TRUE);

propensity <- rnorm(n = population.size, mean = 0.25, sd = 0.025);
is.high.propensity <- (c2 - c1 == 1 | c2 - c1 == -1);
propensity[is.high.propensity] <- rnorm(n = sum(is.high.propensity), mean = 0.75, sd = 0.025);

sigma <- 0.20;
x1 <- c1 + rnorm(n = population.size, mean = 0, sd = sigma);
x2 <- c2 + rnorm(n = population.size, mean = 0, sd = sigma);

y0 <- rep(x = 30, times = population.size);
y0[is.high.propensity] <- 110;

epsilon <- rnorm(n = population.size, mean = 0, sd = 1.0)
y <- y0 + epsilon^2;

DF.population <- data.frame(
    ID         = seq(1,population.size),
    y          = y,
    x1.numeric = x1,
    x2.numeric = x2,
    propensity = propensity
    );

for ( colname.numeric in c("x1.numeric","x2.numeric") ) {

    temp.quantiles <- quantile(
        x     = DF.population[,colname.numeric],
        probs = c(1,2,3)/3
        );

    is.small  <- ifelse(DF.population[,colname.numeric] <  temp.quantiles[1],TRUE,FALSE);
    is.medium <- ifelse(DF.population[,colname.numeric] >= temp.quantiles[1] & DF.population[,colname.numeric] < temp.quantiles[2],TRUE,FALSE);
    is.large  <- ifelse(DF.population[,colname.numeric] >= temp.quantiles[2],TRUE,FALSE);

    colname.factor <- gsub(x = colname.numeric, pattern = "\\.numeric", replacement = "");
    DF.population[,colname.factor] <- character(nrow(DF.population));

    if ( "x1.numeric" == colname.numeric ) {
        DF.population[is.small, colname.factor] <- "small";
        DF.population[is.medium,colname.factor] <- "medium";
        DF.population[is.large, colname.factor] <- "large";
        temp.levels <- c("small","medium","large");
    } else {
        DF.population[is.small, colname.factor] <- "petit";
        DF.population[is.medium,colname.factor] <- "moyen";
        DF.population[is.large, colname.factor] <- "grand";
        temp.levels <- c("petit","moyen","grand");
        }

    DF.population[,colname.factor] <- factor(
        x       = DF.population[,colname.factor],
        levels  = temp.levels,
        ordered = TRUE
        );

    colname.jitter <- gsub(x = colname.numeric, pattern = "numeric", replacement = "jitter");
    DF.population[,colname.jitter] <- (-0.5) + as.numeric(DF.population[,colname.factor]) + runif(n = nrow(DF.population), min = -0.3, max = 0.3);

    DF.population <- DF.population[,setdiff(colnames(DF.population),colname.numeric)];

    }
```

We examine the structure of the data frame DF.training:
```{r}
knitr::kable(head(DF.population), align = "c")
```

```{r, fig.height = 10, fig.width = 9.75}
textsize.title <- 30;
textsize.axis  <- 20;

my.ggplot <- ggplot(data = NULL) + theme_bw();
my.ggplot <- my.ggplot + theme(
    title            = element_text(size = textsize.title, face = "bold"),
    axis.text.x      = element_text(size = textsize.axis,  face = "bold", angle = 0, vjust = 0.5, hjust = 0.5),
    axis.text.y      = element_text(size = textsize.axis,  face = "bold"),
    axis.title.x     = element_text(size = textsize.axis,  face = "bold"),
    axis.title.y     = element_text(size = textsize.axis,  face = "bold"),
    legend.title     = element_text(size = textsize.axis,  face = "bold"),
    legend.text      = element_text(size = textsize.axis,  face = "bold"),
    panel.grid.major = element_line(colour = "gray", linetype = 2, size = 0.25),
    panel.grid.minor = element_line(colour = "gray", linetype = 2, size = 0.25),
    legend.position  = "bottom",
    legend.key.width = ggplot2::unit(0.75,"in")
    );

my.ggplot <- my.ggplot + labs(
    title    = NULL,
    subtitle = "Population",
    colour   = "true propensity       "
    );

my.ggplot <- my.ggplot + geom_hline(yintercept = 0, colour = "gray", size = 0.75);
my.ggplot <- my.ggplot + geom_vline(xintercept = 0, colour = "gray", size = 0.75);

my.ggplot <- my.ggplot + scale_x_continuous(
    limits = c(0,3),
    breaks = c(0.5,1.5,2.5),
    labels = levels(DF.population[,'x1'])
    );
my.ggplot <- my.ggplot + scale_y_continuous(
    limits = c(0,3),
    breaks = c(0.5,1.5,2.5),
    labels = levels(DF.population[,'x2'])
    );

my.ggplot <- my.ggplot + xlab("x1 (jittered)");
my.ggplot <- my.ggplot + ylab("x2 (jittered)");

my.ggplot <- my.ggplot + scale_colour_gradient(
    limits = c(0,1),
    breaks = c(0,0.25,0.5,0.75,1),
    low    = "black",
    high   = "red"
    );

my.ggplot <- my.ggplot + geom_point(
    data    = DF.population,
    mapping = aes(x = x1.jitter, y = x2.jitter, colour = propensity),
    alpha   = 0.2
    );

my.ggplot;
```

```{r}
prob.selection <- 0.1;

is.selected <- sample(
    x       = c(TRUE,FALSE),
    size    = nrow(DF.population),
    replace = TRUE,
    prob    = c(prob.selection, 1 - prob.selection)
    );

DF.probability <- DF.population[is.selected,c("ID","x1","x2")];
DF.probability[,"design.weight"] <- 1 / prob.selection;
```

```{r}
knitr::kable(head(DF.probability), align = "c")
```

```{r}
DF.non.probability <- DF.population;
DF.non.probability[,"self.selected"] <- sapply(
    X   = DF.non.probability[,"propensity"],
    FUN = function(x) { sample(x = c(FALSE,TRUE), size = 1, prob = c(1-x,x)) }
    );
DF.non.probability <- DF.non.probability[DF.non.probability[,"self.selected"],c("ID","y","x1","x2","x1.jitter","x2.jitter")];
```

```{r}
knitr::kable(head(DF.non.probability), align = "c")
```

```{r}
my.nppCART <- nppR::nppCART(
    np.data    = DF.non.probability,
    p.data     = DF.probability,
    predictors = c("x1","x2"),
    weight     = "design.weight"
    );
```

```{r}
my.nppCART$grow();
```

```{r}
my.nppCART$print(
    FUN.format = function(x) {return( round(x,digits=3) )}
    );
```

```{r}
DF.npdata.estimated.propensity <- my.nppCART$get_npdata_with_propensity();
```

```{r}
knitr::kable(head(DF.npdata.estimated.propensity), align = "c")
```

```{r}
colnames(DF.npdata.estimated.propensity) <- gsub(
    x           = colnames(DF.npdata.estimated.propensity),
    pattern     = "^propensity$",
    replacement = "estimated.propensity"
    );
DF.npdata.estimated.propensity <- merge(
    x  = DF.npdata.estimated.propensity,
    y  = DF.population[,c("ID","propensity")],
    by = "ID"
    );
colnames(DF.npdata.estimated.propensity) <- gsub(
    x           = colnames(DF.npdata.estimated.propensity),
    pattern     = "^propensity$",
    replacement = "true.propensity"
    );
DF.npdata.estimated.propensity <- DF.npdata.estimated.propensity[order(DF.npdata.estimated.propensity[,"ID"]),];
```

```{r}
knitr::kable(head(DF.npdata.estimated.propensity), align = "c")
```

```{r}
str(    DF.npdata.estimated.propensity);
summary(DF.npdata.estimated.propensity);
```

```{r, fig.height = 10, fig.width = 9.75}
my.ggplot <- ggplot(data = NULL) + theme_bw();
my.ggplot <- my.ggplot + theme(
    title            = element_text(size = textsize.title, face = "bold"),
    axis.text.x      = element_text(size = textsize.axis,  face = "bold", angle = 0, vjust = 0.5, hjust = 0.5),
    axis.text.y      = element_text(size = textsize.axis,  face = "bold"),
    axis.title.x     = element_text(size = textsize.axis,  face = "bold"),
    axis.title.y     = element_text(size = textsize.axis,  face = "bold"),
    legend.title     = element_text(size = textsize.axis,  face = "bold"),
    legend.text      = element_text(size = textsize.axis,  face = "bold"),
    panel.grid.major = element_line(colour = "gray", linetype = 2, size = 0.25),
    panel.grid.minor = element_line(colour = "gray", linetype = 2, size = 0.25),
    legend.position  = "bottom",
    legend.key.width = ggplot2::unit(0.75,"in")
    );

my.ggplot <- my.ggplot + labs(
    title    = NULL,
    subtitle = "Non-probability sample",
    colour   = "estimated propensity       "
    );

my.ggplot <- my.ggplot + geom_hline(yintercept = 0,colour="gray",size=0.75);
my.ggplot <- my.ggplot + geom_vline(xintercept = 0,colour="gray",size=0.75);

my.ggplot <- my.ggplot + scale_x_continuous(
    limits = c(0,3),
    breaks = c(0.5,1.5,2.5),
    labels = levels(DF.population[,'x1'])
    );
my.ggplot <- my.ggplot + scale_y_continuous(
    limits = c(0,3),
    breaks = c(0.5,1.5,2.5),
    labels = levels(DF.population[,'x2'])
    );

my.ggplot <- my.ggplot + xlab("x1 (jittered)");
my.ggplot <- my.ggplot + ylab("x2 (jittered)");

my.ggplot <- my.ggplot + scale_colour_gradient(
    limits = c(0,1),
    breaks = c(0,0.25,0.5,0.75,1),
    low    = "black",
    high   = "red"
    );

my.ggplot <- my.ggplot + geom_point(
    data    = DF.npdata.estimated.propensity,
    mapping = aes(x = x1.jitter, y = x2.jitter, colour = estimated.propensity),
    alpha   = 0.2
    );

my.ggplot;
```

```{r, fig.height = 10, fig.width = 9.75}
my.ggplot <- ggplot(data = NULL) + theme_bw();
my.ggplot <- my.ggplot + theme(
    title            = element_text(size = textsize.title, face = "bold"),
    axis.text.x      = element_text(size = textsize.axis,  face = "bold"),
    axis.text.y      = element_text(size = textsize.axis,  face = "bold"),
    axis.title.x     = element_text(size = textsize.axis,  face = "bold"),
    axis.title.y     = element_text(size = textsize.axis,  face = "bold"),
    legend.title     = element_text(size = textsize.axis,  face = "bold"),
    legend.text      = element_text(size = textsize.axis,  face = "bold"),
    panel.grid.major = element_line(colour="gray", linetype=2, size=0.25),
    panel.grid.minor = element_line(colour="gray", linetype=2, size=0.25),
    legend.position  = "bottom",
    legend.key.width = ggplot2::unit(1.0,"in")
    );

my.ggplot <- my.ggplot + labs(
    title    = NULL,
    subtitle = "Non-probability sample"
    );

my.ggplot <- my.ggplot + xlab("true propensity");
my.ggplot <- my.ggplot + ylab("estimated propensity");

my.ggplot <- my.ggplot + geom_hline(yintercept = 0, colour = "gray", size = 0.75);
my.ggplot <- my.ggplot + geom_vline(xintercept = 0, colour = "gray", size = 0.75);
my.ggplot <- my.ggplot + scale_x_continuous(limits = c(0,1), breaks = seq(0,1,0.2));
my.ggplot <- my.ggplot + scale_y_continuous(limits = c(0,1), breaks = seq(0,1,0.2));

my.ggplot <- my.ggplot + scale_fill_gradient(
    limits = 260 * c(  0,1),
    breaks = 260 * seq(0,1,0.25),
    low    = "lightgrey",
    high   = "red"
    );

my.ggplot <- my.ggplot + geom_hex(
    data     = DF.npdata.estimated.propensity,
    mapping  = aes(x = true.propensity, y = estimated.propensity),
    binwidth = c(0.02,0.02)
    );

my.ggplot;
```

Each row corresponds to a (reference year, parcel).

As the input data frame to the function
**stcCropYield::rollingWindowForwardValidation**
it must contain the following mandatory columns with the expected data types
(while column names and ordering do NOT matter):

* a numeric column with non-negative integers indicating the reference year
  (in case of DF.training, **my_year**)

* a character column indicating the ecoregion (**my_ecoregion**)

* a character column indicating the crop type (**my_crop**)

* a numeric column with non-negative values indicating the harvested area
  (**my_harvested_area**).
  This is used to compute parcel-level actual crop production
  (actual yield x harvested area).

* a numeric column with non-negative values indicating the seeded area
  (**my_seeded_area**).
  This is used to compute parcel-level predicted crop production
  (predicted yield x seeded area).

* a numeric column with non-negative values indicating the parcel-level
  evaluation weight (**my_evaluation_weight**).
  This is used as a parcel-level (importance) weight
  in the hyperparameter tuning objective function.

* a numeric column with non-negative values indicating
  the target variable, i.e. crop yield (**my_yield**).
  The usual physical unit for crop yield is *number of bushels per acre*.

* a collection of numeric columns of predictor variables
  (in case of DF.training, **x1**, **x2**, ... , **x7**).
  In practice, these could include (but are not limited to)
  the weekly NDVI measurements,
  the weekly (or monthly) weather measurements, and
  their derived variables (e.g. rolling averages, maxima, emerging week, etc).
  There is no limit on the number of predictor variables
  (apart from the available amount of computer memory).

Here are the first few rows of DF.training:
```{r trainingHead}
# knitr::kable(head(DF.training), align = "c")
```
The first few rows of DF.production (note the absence of
**my_yield**, **my_harvested_area**, **my_seeded_area**, and
**my_evaluation_weight**):
```{r productionHead}
# knitr::kable(head(DF.production), align = "c")
```

### Step 1. Train model using **stcCropYield::rollingWindowForwardValidation(.)**

This is the first step in the real production setting, namely to train a
crop yield prediction model based on historical training data
(DF.training, in the present example).

We train the model by calling
**stcCropYield::rollingWindowForwardValidation(.)**:

```{r}
# stcCropYield::rollingWindowForwardValidation(
#     training.window      = 5,
#     validation.window    = 5,
#     DF.input             = DF.training,
#     year                 = "my_year",
#     ecoregion            = "my_ecoregion",
#     crop                 = "my_crop",
#     response.variable    = "my_yield",
#     harvested.area       = "my_harvested_area",
#     seeded.area          = "my_seeded_area",
#     evaluation.weight    = "my_evaluation_weight",
#     predictors           = c("x1","x2","x3","x4","x5","x6","x7"),
#     min.num.parcels      = 50,
#     learner              = "xgboost_multiphase",
#     by.variables.phase01 = c("my_ecoregion","my_crop"),
#     by.variables.phase02 = c("my_crop"),
#     by.variables.phase03 = c("my_ecoregion"),
#     search.grid = list(
#         alpha  = c(1,12,23),
#         lambda = c(1,12,23)
#         ),
#     output.directory = "rwFV",
#     log.threshold    = logger::ERROR,
#     suppress.child.process.graphics = TRUE
#     )
```

*   training.window = 5, validation.window = 5

    These two input parameters together stipulates the use of a
    *rolling window forward validation* scheme
    with a training window of 5 years, and a validation window of 5 years.

*   DF.input = DF.training

    The training data frame is DF.training

*   **stcCropYield::rollingWindowForwardValidation(.)**
    expects its input data frame to have a number of mandatory variables.
    The input parameters
    **year**,
    **ecoregion**,
    **crop**,
    **response.variable**,
    **harvested.area**,
    **seeded.area**,
    **evaluation.weight**,
    **predictors**
    indicate the column names in the input data frame
    of these mandatory variables.

*   min.num.parcels = 50

    For each round of modeling fitting, the minimal numnber of parcels
    required in the corresponding training data is 50.
    If there are fewer records, fitting is suppressed.

*   learner = "xgboost_multiphase"

    "xgboost_multiphase" will be the learner.
    Currently, this learner has 3 phases.

*   by.variables.phase01 = c("my_ecoregion","my_crop")

    In the first phase, a separate model is fitted for each combination
    of ecoregion and crop.

*   by.variables.phase02 = c("my_crop")

    In the second phase, a separate model is fitted for each crop type.

*   by.variables.phase03 = c("my_ecoregion")

    In the third and last phase, a separate model is fitted for each ecoregion.

*   search.grid = list(alpha = c(1,12,23), lambda = c(1,12,23))

    We are tuning on two XGBoost(Linear) parameters, namely, alpha and lambda.
    For each parameter, the candidate values are 1, 12, and 23.
    The full search grid therefore consists of 9 = 3 x 3 candidate
    hyperparameter configurations, namely:
    (alpha, lambda) = (1,1), (1,12), (1,23), (12,1), (12,12), (12,23),
    (23,1), (23,12), (23,23).

*   output.directory = "rwFV"

    All output and log files are to be written to the output folder "rwFV".

*   log.threshold = logger::ERROR

    log.threshold is set to logger::ERROR.
    See documentation of the **logger** package for the other possible
    log.threshold values.

*   suppress.child.process.graphics = TRUE

    In practice, omit this optinal parameter (whose default value is FALSE).
    It is set to TRUE in this vignette only because generating graphics
    in a multi-threaded process during the vignette rendering process
    generated an error.

We next examine the contents in the output direcotry **rwFV**:
```{r outputDIR}
# list.files("rwFV")
```

The output directory *rwFV* contains:

* three sub-directories:
  * 010-predictions
  * 020-performance-metrics
  * 030-mock-productions

* one RData file: **`r list.files(path = "rwFV", pattern = "\\.RData$")`**

* one log file: **rollingWindowForwardValidation.log**

The sub-directory **rwFV/010-predictions** contains the
(hyperparameter,year)-specific parcel-level predictions
as well as rolled up values to the levels of (ecoregion,crop), crop and
ecoregion.

The sub-directory **rwFV/020-performance-metrics** contains the computed performance
metrics according to the aforementioned predictions.

The sub-directory **rwFV/030-productions** contains the prediction error series
in mock production.

The RData file **`r list.files(path = "rwFV", pattern = "\\.RData$")`**
contains the final trained model
that can be deployed in the next production cycle,
and it will be used in Step 2 in the call to
**stcCropYield::crop.yield.predict(.)**.

### Step 2. Use the trained model to compute parcel-level crop yield predictions based on observed data on predictor variables for current production cycle, using **stcCropYield::crop.yield.predict(.)**

The trained model was saved file in Step 1, which is probably performed
in the early months of the reference year, e.g. January or February,
soon after the crop yield insurance data for the preceding reference year
became available.

First, we capture the file path of the previously saved trained model in
a variable:

```{r}
# RData.trained.model <- list.files( path = "rwFV", pattern = "\\.RData$" );
# (RData.trained.model <- file.path("rwFV",RData.trained.model))
```

The (synthetic) data frame **DF.production** generated in Step 0 contains
data for the predictor variables for the current reference year, and will be
supplied to the trained model in order to generate the parcel-level
crop yield predictions. This is done by executing the following:

```{r}
# DF.predictions <- stcCropYield::crop.yield.predict(
#    trained.model = RData.trained.model,
#    DF.predictors = DF.production,
#    # CSV.output  = "2021-production-folder/2021-crop-yield-predictions.csv"
#    );
```

*   The return value of **stcCropYield::crop.yield.predict(.)** is
    the data frame containing the parcel-level crop yield predictions,
    assigned to the variable **DF.predictions** in the code segment above.

*   When the optional input parameter **CSV.output** is supplied, the function
    **stcCropYield::crop.yield.predict(.)**
    will in addition save the output data frame to file accordingly.

We next examine the first few rows of **DF.predictions**:
```{r predictionHead}
# knitr::kable(head(DF.predictions), align = "c")
```
Note that **DF.predictions** is in fact **DF.production** with five
additional columns augmented at the right:

*   predicted_response_phase01: Phase 1 predicted crop yield

*   predicted_response_phase02: Phase 2 predicted crop yield

*   predicted_response_phase03: Phase 3 predicted crop yield

*   predicted_response: final predicted crop yield,
    which is the first non-NA value in the ordered sequence
    (predicted_response_phase01, predicted_response_phase02, predicted_response_phase03)

*   phase: the phase that the final predicted value comes from

Next, we examine the number of records for each phase:
```{r}
# knitr::kable(table(DF.predictions[,'phase']), align = "c")
```

Here are the first 50 records of **DF.predictions** with phase > 1:
```{r}
# DF.predictions.phaseGt01 <- DF.predictions[DF.predictions[,'phase'] > 1,]
# knitr::kable(DF.predictions.phaseGt01[1:50,], align = "c")
```
